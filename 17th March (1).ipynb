{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd72cc43-d743-4182-a71a-2ddc94dd4e84",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Missing values in a dataset refer to the absence of data in a particular observation or feature. There can be various reasons for missing values, such as incomplete data collection, errors during data entry or processing, or intentional missing values\n",
    "\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can affect the quality of the analysis or modeling results. Missing values can lead to biased or inaccurate estimates of model parameters, reduce the statistical power of the analysis, and potentially invalidate the conclusions drawn from the data.\n",
    "\n",
    "\n",
    "Some of the algorithms that are not affected by missing values include decision trees, random forests, and k-nearest neighbors (KNN). These algorithms can handle missing values without imputation by adapting their similarity or distance measures based on the available features or by using surrogate splits in decision trees. Bayesian networks and neural networks can also handle missing values through imputation or probabilistic inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa029b83-75ea-46e4-b766-3dcb0c3b142b",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "Deletion: In this technique, the rows or columns with missing data are removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f791a6f0-284c-4269-8e4b-142ed83ab320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8], 'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b31670-aa07-4396-b566-4dfbea8f6d59",
   "metadata": {},
   "source": [
    "Imputation: In this technique, the missing values are replaced with estimated values based on the available data. The most common imputation methods include mean imputation, median imputation, and mode imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8777fc1-3b22-4016-9fe1-eefc961c90b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B     C\n",
      "0  1.000000  5.000000   9.0\n",
      "1  2.000000  6.666667  10.0\n",
      "2  2.333333  7.000000  11.0\n",
      "3  4.000000  8.000000  12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8], 'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(imputed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716df7cb-ed42-466a-af57-1529fec7c06a",
   "metadata": {},
   "source": [
    "Prediction: In this technique, a model is trained on the available data to predict the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a45f56-6722-446c-9805-ff76ca2ae735",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Imbalanced data refers to a situation in which the distribution of classes or categories in a dataset is not equal. In other words, one class has significantly fewer observations than another class, resulting in an uneven or skewed distribution. For example, in a binary classification problem, if the positive class (e.g., a rare disease) has only a few observations compared to the negative class (e.g., healthy individuals), the dataset is considered imbalanced.\n",
    "\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased models, inaccurate predictions, and poor performance metrics. Machine learning algorithms are designed to optimize the overall accuracy, and in the case of imbalanced data, they tend to favor the majority class, ignoring the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bb8fc-428d-48cd-9f16-8dc58c2814a1",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\n",
    "\n",
    "Up-sampling involves increasing the number of samples in the minority class to match the number of samples in the majority class. This can be done by randomly duplicating existing samples or generating new synthetic samples using techniques such as SMOTE or ADASYN.\n",
    "\n",
    "Down-sampling, on the other hand, involves reducing the number of samples in the majority class to match the number of samples in the minority class. This can be done by randomly selecting a subset of the majority class samples or by clustering the majority class samples and selecting representative sample.\n",
    "\n",
    "\n",
    "For example, suppose we are working on a fraud detection problem, and we have a dataset with 95% non-fraudulent transactions and only 5% fraudulent transactions. In this case, we can up-sample the minority class by generating synthetic samples using SMOTE or ADASYN to balance the dataset. Alternatively, we can down-sample the majority class by randomly selecting a subset of non-fraudulent transactions to match the number of fraudulent transactions. Both techniques can improve the performance of the model by creating a more balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3248e-1754-42fb-aaac-2d0d66a29d72",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by creating new, synthetic data points based on the existing data. It is commonly used in machine learning to improve the performance of models, especially when the available dataset is limited.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation technique specifically designed to address the problem of imbalanced datasets. It involves creating new synthetic samples of the minority class by interpolating between existing samples.\n",
    "\n",
    "\n",
    "The SMOTE technique has been shown to be effective in improving the performance of models on imbalanced datasets, especially in combination with other techniques such as under-sampling and cost-sensitive learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6079ec-443b-447b-a08b-30be27d55dd5",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Outliers in a dataset are data points that are significantly different from the other data points in the dataset.\n",
    "\n",
    "It is essential to handle outliers because they can affect the accuracy and reliability of statistical analysis and machine learning models. Outliers can impact the mean and standard deviation of the data, which in turn can affect the performance of many machine learning algorithms that are sensitive to these statistical measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37929ec-c9d2-4ed8-b616-50cd1c260f9b",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "\n",
    "There are several techniques that can be used to handle missing data in an analysis. The choice of technique depends on the nature of the data and the amount of missing data. Here are some common techniques:\n",
    "\n",
    "Deletion: This technique involves deleting the rows or columns that contain missing data. If the amount of missing data is small, this technique can be used without significantly affecting the analysis. However, if the amount of missing data is large, this technique can lead to bias in the results.\n",
    "\n",
    "Imputation: This technique involves filling in the missing data with estimated values. There are several methods for imputing missing data, including mean imputation, median imputation, mode imputation, and regression imputation.\n",
    "\n",
    "Prediction modeling: This technique involves using a machine learning algorithm to predict the missing values based on the values of the other variables in the dataset.\n",
    "\n",
    "Multiple imputation: This technique involves creating multiple imputed datasets, each with different estimates for the missing values, and then combining the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef3155-136a-4aac-b197-099283a1981f",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "\n",
    "There are several strategies that can be used to determine if missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "Visual inspection: One strategy is to visually inspect the data to see if there is any apparent pattern to the missing data. This can be done using scatter plots, histograms, and other graphical tools.\n",
    "\n",
    "Statistical tests: Another strategy is to use statistical tests to determine if there is a significant difference between the missing and non-missing data. This can include tests for differences in means, variances, or other measures of central tendency or dispersion.\n",
    "\n",
    "Missing data imputation: Another strategy is to impute the missing data using various methods such as mean imputation, median imputation or regression-based imputation. Then compare the imputed values with the observed values and examine if they differ significantly.\n",
    "\n",
    "Pattern recognition algorithms: One could also use pattern recognition algorithms to identify patterns in the data that could explain why certain data points are missing.\n",
    "\n",
    "Domain knowledge: Finally, domain knowledge can be used to help determine if the missing data is likely to be missing at random or if there is a pattern to the missing data based on prior experience or expert knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d301366f-f07e-41c9-b6f1-ac1749ddb523",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64973c03-57ec-4e13-83cf-a021c0b24b87",
   "metadata": {},
   "source": [
    "\n",
    "There are several strategies that can be used to evaluate the performance of machine learning models on imbalanced datasets:\n",
    "\n",
    "Confusion Matrix: The confusion matrix can be used to calculate the True Positive Rate (TPR) or Sensitivity and the True Negative Rate (TNR) or Specificity.\n",
    "\n",
    "Precision-Recall Curve: The precision-recall curve can be used to evaluate the trade-off between precision and recall for different classification thresholds. This can be particularly useful when the positive class is rare.\n",
    "\n",
    "ROC Curve: The Receiver Operating Characteristic (ROC) curve can be used to evaluate the trade-off between sensitivity and specificity for different classification thresholds. This curve is also useful when the positive class is rare.\n",
    "\n",
    "Cost-Sensitive Learning: Cost-sensitive learning algorithms can be used to assign different costs to misclassification errors depending on the class imbalance. This can help the algorithm to focus on the minority class and achieve better performance.\n",
    "\n",
    "Resampling Techniques: Resampling techniques can be used to balance the dataset by either up-sampling the minority class or down-sampling the majority class.\n",
    "\n",
    "Ensemble Methods: Ensemble methods can be used to combine multiple models to improve performance on imbalanced datasets. This can include methods such as bagging, boosting, or stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efb167-a357-4366-b474-8b442465e2d3",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "\n",
    "To balance an imbalanced dataset where the majority class is down-sampled, you can use the following methods:\n",
    "\n",
    "Random Under-sampling: In this method, we randomly remove some of the samples from the majority class to make the dataset balanced. This method may lead to the loss of important information, so it should be used with caution.\n",
    "\n",
    "Cluster Centroids: This method replaces the majority class samples with centroids based on the clustering algorithm.\n",
    "\n",
    "Tomek Links: Tomek links are pairs of samples that are nearest neighbors but belong to different classes. Removing the majority class sample from these pairs helps to separate the classes.\n",
    "\n",
    "Edited Nearest Neighbors: In this method, the majority class samples that are misclassified by the nearest neighbors are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a6c1c-60c0-41b1-a436-f07b3bec858e",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "If the dataset is unbalanced with a low percentage of occurrences of a rare event, one can use the following methods to balance the dataset and up-sample the minority class:\n",
    "\n",
    "Random Oversampling: Randomly selecting and duplicating instances from the minority class until it is balanced with the majority class.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): SMOTE works by creating synthetic samples from the minority class instead of randomly duplicating them. Synthetic samples are created by choosing two or more similar instances from the minority class and creating new synthetic instances along the line joining these instances in the feature space.\n",
    "\n",
    "ADASYN (Adaptive Synthetic Sampling): ADASYN is similar to SMOTE, but it generates more synthetic samples near the boundary of the minority class to improve the classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645010fb-53b6-4c0d-bde2-42a99d316beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
