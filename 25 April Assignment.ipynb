{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d26c69-746c-469d-86db-0aecc85a2b61",
   "metadata": {},
   "source": [
    "Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "Explain with an example.\n",
    "\n",
    "Eigenvalues and eigenvectors are concepts from linear algebra that are closely related to the eigen-decomposition approach in matrix analysis.\n",
    "\n",
    "In the context of a square matrix, an eigenvector is a non-zero vector that, when multiplied by the matrix, produces a scaled version of itself. The scaling factor is called the eigenvalue. Mathematically, for a matrix A, an eigenvector v and its corresponding eigenvalue λ satisfy the equation Av = λv.\n",
    "\n",
    "The eigen-decomposition approach involves decomposing a square matrix into a set of eigenvectors and eigenvalues. This decomposition can be represented as A = PDP^(-1), where A is the original matrix, P is a matrix whose columns are the eigenvectors, and D is a diagonal matrix with eigenvalues on the diagonal.\n",
    "\n",
    "To illustrate this, consider a 2x2 matrix A:\n",
    "\n",
    "A = [3 1]\n",
    "[1 3]\n",
    "\n",
    "To find the eigenvectors and eigenvalues, we solve the equation Av = λv:\n",
    "\n",
    "For the matrix A, the characteristic equation is det(A - λI) = 0, where I is the identity matrix:\n",
    "\n",
    "|3-λ 1 | |3-λ|\n",
    "| | - λ | |\n",
    "|1 | |3-λ|\n",
    "\n",
    "Expanding the determinant gives (3-λ)(3-λ) - 1 = 0:\n",
    "\n",
    "(3-λ)^2 - 1 = 0\n",
    "λ^2 - 6λ + 8 = 0\n",
    "(λ - 4)(λ - 2) = 0\n",
    "\n",
    "Solving this equation, we find the eigenvalues to be λ1 = 4 and λ2 = 2.\n",
    "\n",
    "To find the corresponding eigenvectors, we substitute each eigenvalue back into the equation Av = λv and solve for v:\n",
    "\n",
    "For λ1 = 4:\n",
    "\n",
    "(3-4)v1 + v2 = 0\n",
    "-v1 + v2 = 0\n",
    "v1 = v2\n",
    "\n",
    "Therefore, one eigenvector is [1 1].\n",
    "\n",
    "For λ2 = 2:\n",
    "\n",
    "(3-2)v1 + v2 = 0\n",
    "v1 + v2 = 0\n",
    "v1 = -v2\n",
    "\n",
    "Therefore, another eigenvector is [-1 1].\n",
    "\n",
    "So, the eigenvalues for matrix A are λ1 = 4 and λ2 = 2, and the corresponding eigenvectors are v1 = [1 1] and v2 = [-1 1].\n",
    "\n",
    "Using the eigen-decomposition approach, we can express matrix A as:\n",
    "\n",
    "A = PDP^(-1)\n",
    "\n",
    "where P = [1 -1; 1 1] (eigenvectors matrix) and D = [4 0; 0 2] (diagonal matrix with eigenvalues).\n",
    "\n",
    "The eigen-decomposition approach allows us to understand the relationship between a matrix, its eigenvectors, and eigenvalues. It can be used for various applications, including dimensionality reduction, image processing, and solving systems of differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc13a3-9e96-4d9f-9904-c387880971f8",
   "metadata": {},
   "source": [
    "Q2. What is eigen decomposition and what is its significance in linear algebra?\n",
    "\n",
    "\n",
    "ChatGPT\n",
    "Eigen decomposition is a technique in linear algebra that decomposes a square matrix into a set of eigenvectors and eigenvalues. It is also known as eigendecomposition or spectral decomposition.\n",
    "\n",
    "Given a square matrix A, the eigen decomposition expresses A as the product of three matrices: A = PDP^(-1), where P is a matrix consisting of eigenvectors of A, D is a diagonal matrix containing the corresponding eigenvalues, and P^(-1) is the inverse of P.\n",
    "\n",
    "The significance of eigen decomposition in linear algebra lies in its ability to provide insights into the properties of a matrix. Some key aspects of eigen decomposition include:\n",
    "\n",
    "Eigenvalues: The eigenvalues obtained from eigen decomposition represent the scaling factors by which the corresponding eigenvectors are scaled when multiplied by the matrix. They provide information about the matrix's behavior and characteristics, such as its stretch or contraction along the eigenvector directions.\n",
    "\n",
    "Eigenvectors: The eigenvectors obtained from eigen decomposition are vectors that retain their direction but can be scaled by the eigenvalues. They represent the principal directions or axes along which the matrix operates. Eigenvectors provide valuable information about the dominant patterns or structures in the data represented by the matrix.\n",
    "\n",
    "Diagonalization: Eigen decomposition diagonalizes the matrix, transforming it into a diagonal form where the non-zero entries on the diagonal correspond to the eigenvalues. This diagonal form simplifies various matrix operations and calculations, making it easier to analyze and manipulate the matrix.\n",
    "\n",
    "Matrix properties: Eigen decomposition is closely related to important matrix properties, such as symmetry, positive definiteness, and orthogonality. For example, a symmetric matrix can be decomposed into real eigenvalues and orthogonal eigenvectors.\n",
    "\n",
    "Applications: Eigen decomposition finds extensive applications in various fields, including physics, engineering, data analysis, and machine learning. It is used for dimensionality reduction, pattern recognition, solving differential equations, image processing, and many other mathematical operations.\n",
    "\n",
    "Overall, eigen decomposition provides a powerful tool to analyze and understand the properties of matrices, revealing their underlying structure and facilitating further computations and applications in linear algebra and related fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873d918-90fb-4dc8-9947-9e3f368e0085",
   "metadata": {},
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "Eigen-Decomposition approach? Provide a brief proof to support your answer.\n",
    "\n",
    "\n",
    "For a square matrix to be diagonalizable using the eigen-decomposition approach, it must satisfy the following conditions:\n",
    "\n",
    "The matrix must have n linearly independent eigenvectors, where n is the dimension of the matrix. In other words, there must exist a set of n linearly independent vectors that correspond to n distinct eigenvalues.\n",
    "\n",
    "The matrix must have a complete set of eigenvalues. This means that all eigenvalues must be distinct, and there are no repeated eigenvalues.\n",
    "\n",
    "Proof:\n",
    "To show the conditions for diagonalizability, let's consider a square matrix A of dimension n.\n",
    "\n",
    "The matrix must have n linearly independent eigenvectors:\n",
    "Let v_1, v_2, ..., v_n be n eigenvectors of A corresponding to distinct eigenvalues λ_1, λ_2, ..., λ_n, respectively. We can express these eigenvectors as a matrix P, where each eigenvector is a column of P: P = [v_1 v_2 ... v_n].\n",
    "\n",
    "Now, we can calculate the product AP:\n",
    "\n",
    "AP = [Av_1 Av_2 ... Av_n] = [λ_1v_1 λ_2v_2 ... λ_nv_n]\n",
    "\n",
    "Since each v_i is an eigenvector, we have Av_i = λ_iv_i for i = 1, 2, ..., n.\n",
    "\n",
    "Therefore, AP = [λ_1v_1 λ_2v_2 ... λ_nv_n] = [v_1 v_2 ... v_n]Λ,\n",
    "\n",
    "where Λ is a diagonal matrix with the eigenvalues λ_1, λ_2, ..., λ_n on its diagonal.\n",
    "\n",
    "We can rewrite this equation as AP = PΛ.\n",
    "\n",
    "If we pre-multiply both sides by P^(-1), we get P^(-1)AP = Λ.\n",
    "\n",
    "Since P^(-1)AP is a diagonal matrix Λ, it implies that A is similar to a diagonal matrix Λ, which means that A is diagonalizable.\n",
    "\n",
    "The matrix must have a complete set of eigenvalues:\n",
    "If A has repeated eigenvalues, it means that there exist two or more linearly independent eigenvectors corresponding to the same eigenvalue. In this case, the matrix will not be diagonalizable because we cannot form a matrix P with n linearly independent eigenvectors.\n",
    "\n",
    "Therefore, for the eigen-decomposition approach to be applicable and the matrix to be diagonalizable, both conditions of having n linearly independent eigenvectors and a complete set of eigenvalues must be satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7d7e0-1f93-4fae-85b6-1216fa94e1a6",
   "metadata": {},
   "source": [
    "Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "How is it related to the diagonalizability of a matrix? Explain with an example.\n",
    "\n",
    "\n",
    "ChatGPT\n",
    "The spectral theorem is a fundamental result in linear algebra that provides a powerful connection between eigenvalues, eigenvectors, and the diagonalizability of a matrix. It states that for a symmetric matrix, the eigenvalues are real and the eigenvectors corresponding to distinct eigenvalues are orthogonal.\n",
    "\n",
    "The significance of the spectral theorem in the context of the eigen-decomposition approach is that it guarantees the diagonalizability of a symmetric matrix. This means that any symmetric matrix can be expressed as a product of three matrices: A = PDP^T, where P is a matrix whose columns are the eigenvectors of A, and D is a diagonal matrix with the eigenvalues of A on its diagonal.\n",
    "\n",
    "The diagonalizability of a matrix is crucial because it allows us to simplify computations and gain insight into the properties of the matrix. Diagonal matrices are particularly convenient to work with because they have zeros off the diagonal, which simplifies matrix operations such as matrix multiplication, inversion, and exponentiation.\n",
    "\n",
    "Here's an example to illustrate the significance of the spectral theorem and diagonalizability:\n",
    "\n",
    "Consider a symmetric matrix A:\n",
    "\n",
    "A = [[3, 2],\n",
    "[2, 4]]\n",
    "\n",
    "To find the eigenvalues and eigenvectors of A, we solve the characteristic equation: |A - λI| = 0, where I is the identity matrix.\n",
    "\n",
    "|A - λI| = |[[3, 2],\n",
    "[2, 4]] - λ[[1, 0],\n",
    "[0, 1]]| = |[[3-λ, 2],\n",
    "[2, 4-λ]]| = (3-λ)(4-λ) - 2(2) = λ^2 - 7λ + 10 = 0\n",
    "\n",
    "Solving this quadratic equation, we find the eigenvalues λ_1 = 5 and λ_2 = 2.\n",
    "\n",
    "To find the corresponding eigenvectors, we substitute each eigenvalue back into the equation (A - λI)v = 0, where v is the eigenvector.\n",
    "\n",
    "For λ_1 = 5, we have:\n",
    "\n",
    "(A - 5I)v_1 = [[3, 2],\n",
    "[2, 4]]v_1 = 0\n",
    "\n",
    "Solving this system of equations, we find v_1 = [1, -1].\n",
    "\n",
    "For λ_2 = 2, we have:\n",
    "\n",
    "(A - 2I)v_2 = [[3, 2],\n",
    "[2, 4]]v_2 = 0\n",
    "\n",
    "Solving this system of equations, we find v_2 = [1, 1].\n",
    "\n",
    "Now, we can construct the matrix P using the eigenvectors as columns:\n",
    "\n",
    "P = [[1, 1],\n",
    "[-1, 1]]\n",
    "\n",
    "And the diagonal matrix D using the eigenvalues:\n",
    "\n",
    "D = [[5, 0],\n",
    "[0, 2]]\n",
    "\n",
    "Finally, we can verify that A = PDP^T:\n",
    "\n",
    "PDP^T = [[1, 1],\n",
    "[-1, 1]][[5, 0],\n",
    "[0, 2]][[1, -1],\n",
    "[1, 1]] = [[3, 2],\n",
    "[2, 4]] = A\n",
    "\n",
    "This demonstrates that A can be diagonalized using the spectral theorem, and we have expressed A as a product of three matrices: P, D, and P^T. The diagonal form of A allows us to easily compute matrix powers, perform transformations, and understand the geometric properties of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cb66d-1856-4b13-83a9-c5e1abc24d15",
   "metadata": {},
   "source": [
    "Q5. How do you find the eigenvalues of a matrix and what do they represent?\n",
    "\n",
    "\n",
    "To find the eigenvalues of a square matrix, you need to solve the characteristic equation.\n",
    "\n",
    "Given a square matrix A, the characteristic equation is defined as:\n",
    "\n",
    "det(A - λI) = 0,\n",
    "\n",
    "where det() denotes the determinant, A is the matrix, λ is the eigenvalue, and I is the identity matrix of the same size as A.\n",
    "\n",
    "Solving this equation will give you the eigenvalues of the matrix A.\n",
    "\n",
    "The eigenvalues represent the scalar values that characterize the linear transformation defined by the matrix A. Each eigenvalue corresponds to a particular eigenvector, and together they provide information about how the matrix A transforms space.\n",
    "\n",
    "More specifically, the eigenvalues determine the scaling factor by which the corresponding eigenvectors are stretched or compressed during the transformation. If an eigenvector has an eigenvalue of λ, it means that the vector is scaled by a factor of λ when transformed by the matrix A.\n",
    "\n",
    "Eigenvalues have several important properties and applications in linear algebra and various fields of science and engineering. Some key uses of eigenvalues include:\n",
    "\n",
    "Determining matrix properties: Eigenvalues help determine whether a matrix is invertible (non-zero eigenvalues indicate invertibility), diagonalizable (if a matrix has distinct eigenvalues), or singular (if any eigenvalue is zero).\n",
    "\n",
    "Characterizing system behavior: In dynamic systems and control theory, eigenvalues are used to analyze stability and predict the behavior of systems represented by matrices.\n",
    "\n",
    "Dimensionality reduction: Eigenvalues play a crucial role in dimensionality reduction techniques like Principal Component Analysis (PCA). The eigenvalues of the covariance matrix indicate the amount of variance explained by each principal component.\n",
    "\n",
    "Graph theory: Eigenvalues are used to analyze properties of graphs, such as connectivity, centrality, and clustering.\n",
    "\n",
    "By finding and understanding the eigenvalues of a matrix, we gain insight into the fundamental properties and behavior of the associated linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ef939-323a-412e-b46f-9e2071f57cbc",
   "metadata": {},
   "source": [
    "Q6. What are eigenvectors and how are they related to eigenvalues?\n",
    "\n",
    "Eigenvectors are the non-zero vectors associated with eigenvalues in a matrix. In other words, eigenvectors are the vectors that, when multiplied by a square matrix, result in a scaled version of themselves.\n",
    "\n",
    "Given a square matrix A and an eigenvalue λ, an eigenvector v is a non-zero vector that satisfies the equation:\n",
    "\n",
    "Av = λv.\n",
    "\n",
    "Here, A is the matrix, v is the eigenvector, and λ is the corresponding eigenvalue.\n",
    "\n",
    "Eigenvectors are not unique and can be scaled by any non-zero constant. This means that if v is an eigenvector, then any scalar multiple of v (kv, where k is a non-zero scalar) is also an eigenvector corresponding to the same eigenvalue. This property is known as the eigenvector's direction being preserved under the transformation defined by the matrix A.\n",
    "\n",
    "Eigenvectors play a fundamental role in understanding linear transformations and matrix operations. They provide insight into the transformational behavior of a matrix and help us understand how the matrix stretches, compresses, or rotates vectors in a particular direction.\n",
    "\n",
    "The eigenvalues and eigenvectors of a matrix are closely related. Each eigenvalue corresponds to a set of eigenvectors that span an eigenspace associated with that eigenvalue. The eigenvectors associated with the same eigenvalue can be thought of as representing different directions in the transformation defined by the matrix. The magnitude of the eigenvalue determines the scaling factor by which the corresponding eigenvectors are scaled during the transformation.\n",
    "\n",
    "Eigenvalues and eigenvectors are used together to diagonalize a matrix in the eigen-decomposition approach, where the matrix is expressed as a product of eigenvectors and eigenvalues. This decomposition is valuable in various applications, such as solving linear systems, understanding matrix properties, and performing dimensionality reduction techniques like PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b422c-d59d-44d4-9d0f-8c4482a8d6d6",
   "metadata": {},
   "source": [
    "Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n",
    "\n",
    "\n",
    "Certainly! The geometric interpretation of eigenvectors and eigenvalues provides insights into how matrices transform vectors and the directions in which they stretch or compress them.\n",
    "\n",
    "When a matrix acts on an eigenvector, the resulting vector is collinear with the original eigenvector, albeit possibly scaled. The eigenvalue associated with that eigenvector represents the scaling factor. In other words, the eigenvector remains in the same direction but may change in length.\n",
    "\n",
    "Geometrically, the eigenvector represents a direction in space that remains unchanged by the linear transformation represented by the matrix. It can be thought of as an axis of symmetry or a special direction in the transformation.\n",
    "\n",
    "For example, consider a 2x2 matrix A and its eigenvectors v1 and v2, with corresponding eigenvalues λ1 and λ2. If we visualize these eigenvectors as arrows starting from the origin, the matrix A will transform these arrows by scaling them along their respective directions. The magnitude of the eigenvalue determines the scaling factor. Eigenvectors associated with larger eigenvalues will be stretched more, while eigenvectors associated with smaller eigenvalues will be compressed.\n",
    "\n",
    "The eigenvalues themselves provide information about the stretch or compression along the eigenvector directions. If an eigenvalue is positive, it indicates stretching along the corresponding eigenvector. Conversely, if an eigenvalue is negative, it represents compression along the eigenvector. A zero eigenvalue suggests that the transformation collapses the vector along the corresponding eigenvector direction.\n",
    "\n",
    "Overall, the geometric interpretation of eigenvectors and eigenvalues helps us understand the inherent transformational properties of matrices and their impact on vector directions and magnitudes. It allows us to analyze how matrices distort, stretch, rotate, or compress vectors in different directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca358e-8551-4a44-a86a-b227c8fb1c8e",
   "metadata": {},
   "source": [
    "Q8. What are some real-world applications of eigen decomposition?\n",
    "\n",
    "Eigen decomposition has various real-world applications across different fields. Here are a few examples:\n",
    "\n",
    "Image Compression: In image processing, eigen decomposition can be used for image compression techniques like Principal Component Analysis (PCA). By decomposing an image into its eigenvalues and eigenvectors, we can identify the most important features and represent the image using a reduced set of eigenvectors, resulting in efficient compression.\n",
    "\n",
    "Data Analysis and Dimensionality Reduction: Eigen decomposition, particularly through techniques like PCA, is widely used for dimensionality reduction in data analysis. It helps identify the most significant features or variables in a dataset and reduces its dimensionality while preserving the most important information.\n",
    "\n",
    "Signal Processing: Eigen decomposition is employed in signal processing applications such as audio and speech recognition, where it helps extract relevant features from signals. For example, the eigen decomposition of the covariance matrix of audio signals can provide insights into the dominant frequency components or spectral characteristics.\n",
    "\n",
    "Quantum Mechanics: In quantum mechanics, eigen decomposition plays a fundamental role. The eigenvalues and eigenvectors of operators in quantum mechanics represent the possible outcomes and corresponding states of a physical system. They are used to analyze and predict the behavior of quantum systems.\n",
    "\n",
    "Recommendation Systems: Eigen decomposition techniques like Singular Value Decomposition (SVD) are utilized in recommendation systems to analyze user-item interaction matrices. By decomposing these matrices, it becomes possible to identify latent factors or features that contribute to user preferences, which can be used for personalized recommendations.\n",
    "\n",
    "Graph Analysis: Eigen decomposition can be applied to graph-based data, such as social networks or web graphs. It helps identify influential nodes, detect communities, and understand the overall structure of the graph. The eigenvector centrality measure, based on eigen decomposition, is used to identify important nodes in a network.\n",
    "\n",
    "These are just a few examples, and eigen decomposition finds applications in many other areas such as finance, physics, data clustering, and machine learning. Its ability to analyze and extract meaningful information from complex datasets makes it a valuable tool in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373eab6-4444-40a9-9266-1b03f2fbd3d6",
   "metadata": {},
   "source": [
    "Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?\n",
    "\n",
    "No, a matrix cannot have more than one set of eigenvectors and eigenvalues. The eigenvectors and eigenvalues of a matrix are unique for a given matrix, provided that the matrix is square and the eigenvalues are distinct.\n",
    "\n",
    "If a square matrix A has n distinct eigenvalues, then it will have n linearly independent eigenvectors corresponding to those eigenvalues. These eigenvectors span the entire vector space associated with the matrix A. However, it is important to note that the eigenvectors themselves are not unique, as they can be scaled by any nonzero scalar.\n",
    "\n",
    "In some cases, a matrix may have repeated eigenvalues, which means that there may be fewer linearly independent eigenvectors than the number of eigenvalues. This situation is known as eigenvalue multiplicity. In such cases, it is still possible to find a set of linearly independent eigenvectors, but the matrix will not have a complete set of n linearly independent eigenvectors.\n",
    "\n",
    "Overall, each eigenvalue of a matrix corresponds to a unique eigenvector, and the eigenvectors form a linearly independent set associated with the matrix's eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6c1e3-214c-4c47-8a7c-065607b3f7b7",
   "metadata": {},
   "source": [
    "Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.\n",
    "\n",
    "\n",
    "ChatGPT\n",
    "The Eigen-Decomposition approach is highly useful in data analysis and machine learning due to its ability to extract valuable information from data. Here are three specific applications or techniques that rely on Eigen-Decomposition:\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that uses Eigen-Decomposition to identify the principal components of a dataset. By calculating the eigenvectors and eigenvalues of the covariance matrix, PCA determines the directions of maximum variance in the data. These eigenvectors, known as principal components, can then be used to project the data onto a lower-dimensional space while preserving as much information as possible. PCA finds applications in data compression, feature extraction, and visualization.\n",
    "\n",
    "Spectral Clustering: Spectral clustering is a clustering technique that utilizes Eigen-Decomposition to partition data points into groups based on similarity. The process involves constructing an affinity matrix that measures pairwise similarities between data points. By computing the eigenvectors associated with the largest eigenvalues of the affinity matrix, spectral clustering identifies the low-dimensional embedding of the data. Clustering can then be performed on this lower-dimensional representation. Spectral clustering is effective for handling non-linearly separable data and is widely used in image segmentation, community detection in networks, and other clustering tasks.\n",
    "\n",
    "Recommender Systems: Eigen-Decomposition is also employed in collaborative filtering-based recommender systems. In these systems, user-item ratings are often represented in a matrix, with users as rows and items as columns. The matrix is then decomposed using techniques like Singular Value Decomposition (SVD), which is a form of Eigen-Decomposition. By decomposing the matrix, SVD identifies latent factors that capture underlying patterns in the user-item interactions. These factors can be used to make personalized recommendations by predicting missing ratings or estimating user preferences. SVD-based recommender systems are widely used in e-commerce, movie recommendations, and personalized content delivery.\n",
    "\n",
    "In summary, the Eigen-Decomposition approach has diverse applications in data analysis and machine learning. It enables dimensionality reduction, clustering, and personalized recommendations by extracting essential information from data through the calculation of eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c94b05-4196-42c1-9920-1f088e752d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
