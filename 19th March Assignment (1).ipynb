{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cd32cc-733c-4512-8adc-d05ca12c57b4",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Min-Max scaling is a common data preprocessing technique used to rescale numerical features to a common range of values. It transforms each feature such that its minimum value is mapped to 0, and its maximum value is mapped to 1. Min-Max scaling is particularly useful when working with machine learning algorithms that are sensitive to the scale of the input features.\n",
    "\n",
    "\n",
    "For example, suppose we have a dataset containing three features: age, income, and education level. The age feature ranges from 20 to 70, the income feature ranges from 20,000 to 200,000, and the education level feature ranges from 0 to 10. We can use Min-Max scaling to rescale these features to a common range of [0, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1d39f-b160-4728-9689-e118710db4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c4e50b-b77b-4152-bd4e-7aa047a4adf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age scaled: 0.5\n",
      "Income scaled: 0.3333333333333333\n",
      "Education scaled: 0.8\n"
     ]
    }
   ],
   "source": [
    "age = 45\n",
    "income = 80000\n",
    "education = 8\n",
    "\n",
    "age_scaled = (age - 20) / (70 - 20)\n",
    "income_scaled = (income - 20000) / (200000 - 20000)\n",
    "education_scaled = (education - 0) / (10 - 0)\n",
    "\n",
    "print(\"Age scaled:\", age_scaled)\n",
    "print(\"Income scaled:\", income_scaled)\n",
    "print(\"Education scaled:\", education_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ae6a6-e118-4d14-ad4b-935d2d7b597d",
   "metadata": {},
   "source": [
    "Min-Max scaling can also be applied to a dataset with many features, where each feature is rescaled independently. It is important to note that Min-Max scaling is sensitive to outliers, which can skew the range of the scaled data. Therefore, it is recommended to remove or handle outliers before applying Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a5af5-0c1a-4bf8-8276-18ccde9e4753",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "The unit vector technique is a feature scaling method used to transform the values of features to have a magnitude of 1. This is also known as normalization. In other words, the unit vector technique scales each feature so that it lies on the unit circle or hypersphere. It is particularly useful when the magnitude of the data is not important, but only the direction of the data matters.\n",
    "\n",
    "\n",
    "The unit vector scaling formula for a feature x is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb6375-3fc9-4048-8ff9-eaf805a87fef",
   "metadata": {},
   "source": [
    "x_unit_vector = x / ||x||\n",
    "\n",
    "\n",
    "The Euclidean norm of a vector is the square root of the sum of the squared elements of the vector. For a 2D vector (x, y), the Euclidean norm is:\n",
    "\n",
    "||(x, y)|| = sqrt(x^2 + y^2)\n",
    "\n",
    "\n",
    "For a higher-dimensional vector (x1, x2, ..., xn), the Euclidean norm is:\n",
    "\n",
    "\n",
    "||(x1, x2, ..., xn)|| = sqrt(x1^2 + x2^2 + ... + xn^2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30bcc43-5029-489a-b998-8df673323d9f",
   "metadata": {},
   "source": [
    "For example, suppose we have a dataset containing two features: height and weight. We can use unit vector scaling to transform these features into a unit vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7554b3-a579-49ab-8ddc-1944553aae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92847669 0.37139068]\n",
      " [0.91381155 0.40613847]\n",
      " [0.95447998 0.29827499]\n",
      " [0.9486833  0.31622777]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a matrix of height and weight values\n",
    "X = np.array([[175, 70], [180, 80], [160, 50], [165, 55]])\n",
    "\n",
    "# Compute the Euclidean norm of each row\n",
    "norms = np.linalg.norm(X, axis=1)\n",
    "\n",
    "# Compute the unit vector of each row\n",
    "X_unit = X / norms.reshape(-1, 1)\n",
    "\n",
    "print(X_unit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148eb3a7-7485-493f-91d8-5156ed0ff7ab",
   "metadata": {},
   "source": [
    "Compared to Min-Max scaling, unit vector scaling is less sensitive to outliers and can be more useful when the magnitude of the data is not important. However, it may not be suitable for all types of data or algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c688d4-606c-4109-a56b-e8193d752c68",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "PCA, or Principal Component Analysis, is a technique in data science used for dimensionality reduction. It works by identifying patterns and relationships in data by transforming it into a new coordinate system, called the principal components. These principal components are chosen to account for the maximum variance in the data. In other words, PCA is used to transform a high-dimensional dataset into a lower-dimensional dataset, while preserving as much of the variance in the data as possible.\n",
    "\n",
    "PCA is particularly useful when working with datasets with many features or variables, where it can be difficult to visualize or analyze the data. By reducing the number of features or variables, PCA can make the data easier to analyze and visualize, while also reducing the computational complexity of the analysis.\n",
    "\n",
    "Here is an example of how PCA can be used for dimensionality reduction:\n",
    "\n",
    "Suppose we have a dataset of 1000 observations, each with 10 features. We can use PCA to reduce the number of features to 2, while preserving as much of the variance in the data as possible. Here is some sample code to do this using Python and scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f5cc93-18a6-4c4f-90b3-99d4124b9040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1000, 10)\n",
      "Transformed shape: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create a random dataset with 1000 observations and 10 features\n",
    "X = np.random.randn(1000, 10)\n",
    "\n",
    "# Fit PCA to the data, keeping the first 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data into the new coordinate system defined by the principal components\n",
    "X_transformed = pca.transform(X)\n",
    "\n",
    "# Print the shape of the original and transformed datasets\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Transformed shape:\", X_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8861bb-b631-4d1a-b8d3-5508498980a3",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "PCA and Feature Extraction are related concepts in that PCA can be used as a feature extraction technique. Feature extraction is the process of selecting or transforming features in a dataset to improve the performance of a machine learning algorithm. PCA can be used as a feature extraction technique by transforming the original dataset into a new coordinate system, where the new features are the principal components identified by the PCA algorithm.\n",
    "\n",
    "In PCA-based feature extraction, the new features (i.e., the principal components) are selected based on their ability to account for the maximum variance in the data. By selecting only the most informative features, PCA can reduce the dimensionality of the dataset, while still preserving most of the important information.\n",
    "\n",
    "Here is an example of how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset of images of handwritten digits, each represented as a 28x28 matrix of pixel values. Each pixel value ranges from 0 to 255, representing the intensity of the pixel. We can use PCA to extract a set of features from the images that captures the most important information in the data.\n",
    "\n",
    "Here's some sample code to do this using Python and scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f01125-c32d-4cb6-92b4-0ac73536c1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1797, 64)\n",
      "Transformed shape: (1797, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the handwritten digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Extract the features from the dataset using PCA\n",
    "pca = PCA(n_components=20)\n",
    "X_transformed = pca.fit_transform(digits.data)\n",
    "\n",
    "# Print the shape of the original and transformed datasets\n",
    "print(\"Original shape:\", digits.data.shape)\n",
    "print(\"Transformed shape:\", X_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcddccae-7fbb-4734-8f16-221c71f7a9a2",
   "metadata": {},
   "source": [
    "In summary, PCA can be used as a feature extraction technique by transforming the original dataset into a new coordinate system where the new features are the principal components identified by the PCA algorithm. By selecting only the most informative features, PCA can reduce the dimensionality of the dataset while still preserving most of the important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba3c7b-9e12-40d8-a6c4-2753f46f024b",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "\n",
    "Preprocess the data for a recommendation system, we would use Min-Max scaling to transform the numerical features to a common scale, so that they can be used together without being affected by their original range. Here are the steps we can follow to use Min-Max scaling:\n",
    "\n",
    "Identify the numerical features: In this case, the numerical features are price, rating, and delivery time.\n",
    "\n",
    "Determine the range of each feature: We need to determine the minimum and maximum values for each feature in the dataset.\n",
    "\n",
    "Apply Min-Max scaling: For each numerical feature, we apply the Min-Max scaling formula:\n",
    "\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "where value is the original value of the feature, min_value is the minimum value of the feature in the dataset, and max_value is the maximum value of the feature in the dataset.\n",
    "\n",
    "Replace the original values with the scaled values: We replace the original values of each feature with their scaled values.\n",
    "\n",
    "\n",
    "\n",
    "We apply Min-Max scaling to the numerical features using the MinMaxScaler() function from scikit-learn. This scales each feature to a common range of 0 to 1. Finally, we verify that the scaling was successful by printing the minimum and maximum values for each feature after scaling.\n",
    "\n",
    "After preprocessing the data with Min-Max scaling, we can use it to train a recommendation system to predict which food items a user is most likely to order based on their preferences for price, rating, and delivery time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9f489-3ce6-4627-bd2e-f2fca9b730e3",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "In the context of the stock price prediction project, PCA can be used to reduce the dimensionality of the dataset by identifying the most important features that explain the majority of the variance in the data. Here are the steps to apply PCA in this scenario:\n",
    "\n",
    "Standardize the data: Before applying PCA, it is important to standardize the data by subtracting the mean and dividing by the standard deviation. This is done to ensure that all features have the same scale and to remove any bias that might be introduced by differences in the magnitude of the features.\n",
    "\n",
    "Compute the covariance matrix: After standardizing the data, compute the covariance matrix to capture the linear relationship between the features.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues of the covariance matrix can be computed using matrix decomposition techniques. The eigenvectors represent the directions in which the data varies the most, while the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "Select the principal components: The principal components are the eigenvectors with the highest eigenvalues. These represent the most important directions of variation in the data and can be used to project the data onto a lower-dimensional space.\n",
    "\n",
    "Project the data onto the new space: Finally, the data can be projected onto the new space defined by the principal components. This results in a lower-dimensional representation of the data that retains most of the variance.\n",
    "\n",
    "By applying PCA, we can reduce the dimensionality of the dataset while retaining most of the important information. This can help to improve the performance of the model by reducing the risk of overfitting and increasing the interpretability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eacaa7-31b7-4aed-842c-c7bd2d9928c7",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "\n",
    "\n",
    "To perform Min-Max scaling to transform the values to a range of -1 to 1, we can use the following formula:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value) * 2 - 1\n",
    "\n",
    "First, we need to determine the minimum and maximum values in the dataset:\n",
    "\n",
    "min_value = 1\n",
    "max_value = 20\n",
    "\n",
    "Then, we can apply the formula to each value in the dataset:\n",
    "\n",
    "scaled_value_1 = (1 - 1) / (20 - 1) * 2 - 1 = -1\n",
    "scaled_value_5 = (5 - 1) / (20 - 1) * 2 - 1 = -0.6\n",
    "scaled_value_10 = (10 - 1) / (20 - 1) * 2 - 1 = -0.2\n",
    "scaled_value_15 = (15 - 1) / (20 - 1) * 2 - 1 = 0.2\n",
    "scaled_value_20 = (20 - 1) / (20 - 1) * 2 - 1 = 1\n",
    "\n",
    "Therefore, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are [-1, -0.6, -0.2, 0.2, 1].\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be020b0d-2a3f-42ff-871e-c01bcdf61b59",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "\n",
    "Performing feature extraction using PCA involves the following steps:\n",
    "\n",
    "Standardize the dataset by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "Calculate the covariance matrix of the standardized dataset.\n",
    "Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "Sort the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "Choose the top k eigenvectors with the highest eigenvalues to retain as principal components.\n",
    "Transform the original dataset into the new k-dimensional feature space using the retained eigenvectors.\n",
    "The number of principal components to retain depends on the desired level of explained variance and the specific requirements of the problem at hand. One commonly used approach is to retain enough principal components to explain a certain percentage of the total variance in the dataset, such as 90% or 95%. Another approach is to choose the minimum number of principal components that still capture the most important patterns or trends in the data.\n",
    "\n",
    "Without more information about the specific dataset and the requirements of the problem, it is difficult to determine how many principal components to retain. However, in general, it is a good idea to experiment with different numbers of principal components and evaluate the performance of the model on a validation set to determine the optimal number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81607a-c405-476f-b9db-1d3f5195b229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
