{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0608e602-fc54-4c35-9b7c-3939ba4f3c05",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a machine learning algorithm that combines the principles of gradient descent optimization and boosting to build a regression model. It is a powerful technique for solving regression problems and is known for its ability to handle complex nonlinear relationships between features and target variables.\n",
    "\n",
    "In Gradient Boosting Regression, the algorithm builds an ensemble of weak regression models, typically decision trees, in an iterative manner. The process involves the following steps:\n",
    "\n",
    "Initialization: The initial prediction is set as the average or a constant value of the target variable.\n",
    "\n",
    "Iterative training:\n",
    "a. Calculate the gradient of the loss function: The algorithm computes the negative gradient (residuals) of the loss function with respect to the current prediction. The loss function measures the difference between the predicted values and the actual target values.\n",
    "b. Train a weak regression model: A weak regression model, such as a decision tree, is trained to fit the negative gradient (residuals) of the loss function. The weak model aims to capture the patterns and trends in the residuals.\n",
    "c. Update the prediction: The weak model's prediction is multiplied by a learning rate (step size) and added to the current prediction. This step gradually improves the prediction by iteratively reducing the residuals.\n",
    "\n",
    "Repeat steps 2a-2c: Steps 2a-2c are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak regression model is trained to fit the negative gradient of the loss function, and the prediction is updated accordingly.\n",
    "\n",
    "Final prediction: The final prediction is obtained by summing the predictions of all weak regression models. Each weak model's prediction is weighted by a factor that represents its contribution to the ensemble.\n",
    "\n",
    "By iteratively updating the prediction based on the negative gradient of the loss function, Gradient Boosting Regression focuses on reducing the errors made by the previous models in the ensemble. This iterative process enables the model to gradually learn complex relationships and produce accurate predictions.\n",
    "\n",
    "The choice of the loss function depends on the specific regression problem. Commonly used loss functions for Gradient Boosting Regression include mean squared error (MSE) and mean absolute error (MAE), among others.\n",
    "\n",
    "Overall, Gradient Boosting Regression is a powerful algorithm that leverages gradient descent optimization and boosting to create a strong regression model capable of capturing intricate patterns in the data and providing accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9cc0a-7c0e-4699-a240-f11f93029535",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a6f517-f92a-4e7a-9773-55589b2a8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b8fdff-7a22-4c1d-986a-0de40dab95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 36.00002495795783\n",
      "R-squared: -3.5000031197447283\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions with the average target value\n",
    "        initial_prediction = np.mean(y)\n",
    "        predictions = np.full(len(y), initial_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Train a decision tree to fit the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=1)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the predictions by adding the weak model's prediction\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Store the weak model in the ensemble\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "\n",
    "        for tree in self.estimators:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    numerator = np.sum((y_true - y_pred) ** 2)\n",
    "    denominator = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.array([[6], [7]])\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y, model.predict(X))\n",
    "r2 = r_squared(y, model.predict(X))\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25aa1b1-63bb-4f7b-97ba-942221cefeb8",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da2a4d2-ae2d-42b0-8508-f2daa447dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}\n",
      "Mean Squared Error: 7.281471577274377e-13\n",
      "R-squared: 0.999999999999909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create an instance of the GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create a scorer for the GridSearchCV\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=scorer, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = best_model.score(X, y)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafec8e1-dccb-4b80-b4f0-f1026e82b4e8",
   "metadata": {},
   "source": [
    "In this example, we create an instance of GradientBoostingRegressor and define the parameter grid param_grid containing different combinations of hyperparameters. We then create a scorer using the make_scorer function, specifying the mean_squared_error as the metric to optimize. The GridSearchCV class is used to perform the grid search, passing the model, parameter grid, scorer, and cross-validation configuration. The grid search is executed with the fit method on the data X and y. After the search is completed, we obtain the best parameters and best model using the best_params_ and best_estimator_ attributes of the GridSearchCV object.\n",
    "\n",
    "Finally, we evaluate the best model by predicting on the training data and calculating the mean squared error and R-squared metrics.\n",
    "\n",
    "You can modify the parameter grid param_grid with additional values or include other hyperparameters to further optimize the model's performance. Alternatively, you can use RandomizedSearchCV instead of GridSearchCV for random search, which allows you to specify a distribution of values for each hyperparameter to sample from.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd651c5d-06ee-49aa-8c6f-8c7e6bc6f93d",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a simple or \"weak\" model that is relatively simple and has modest predictive power on its own. Weak learners are typically models with low complexity, such as decision trees with few levels or shallow depth.\n",
    "\n",
    "The idea behind Gradient Boosting is to iteratively add these weak learners to the ensemble in order to create a strong learner that can make accurate predictions. Each weak learner focuses on learning and correcting the mistakes or errors made by the previous weak learners in the ensemble.\n",
    "\n",
    "Weak learners are trained sequentially, where each subsequent learner is trained to improve upon the mistakes made by the previous learners. In each iteration, the weak learner is fitted to the residuals or errors of the ensemble's predictions. By repeatedly adding weak learners and adjusting their weights, the ensemble gradually reduces the overall error and improves the predictive performance.\n",
    "\n",
    "It's important to note that although weak learners individually may not perform well, their combination through boosting can result in a powerful ensemble model that achieves high accuracy. The strength of Gradient Boosting lies in its ability to leverage the collective knowledge of many weak learners to make accurate predictions on complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514bd5b-2339-4f4a-83af-f5a4b80a0b84",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "Combining Weak Learners: The Gradient Boosting algorithm aims to combine multiple weak learners (simple models with modest predictive power) to create a strong learner with high predictive accuracy. By sequentially adding weak learners to the ensemble, the algorithm leverages their collective knowledge to improve predictions.\n",
    "\n",
    "Correcting Errors: Each weak learner is trained to correct the mistakes or errors made by the previous weak learners. In other words, the algorithm focuses on reducing the residual errors from the previous iterations. This is achieved by fitting the weak learner to the residuals or gradients of the loss function with respect to the ensemble's predictions.\n",
    "\n",
    "Gradual Improvement: The algorithm iteratively improves the predictions by minimizing the loss function. In each iteration, a weak learner is added to the ensemble and the predictions are updated accordingly. By gradually reducing the errors, the ensemble becomes more accurate over time.\n",
    "\n",
    "Gradient Descent Optimization: The name \"Gradient Boosting\" stems from the use of gradient descent optimization to find the best direction to update the predictions. The negative gradient of the loss function with respect to the current predictions is used as the target for the next weak learner. This ensures that subsequent weak learners focus on the areas where the model performs poorly, allowing the ensemble to improve in those regions.\n",
    "\n",
    "Ensemble of Weak Learners: The final prediction is obtained by combining the predictions of all the weak learners, typically through weighted averaging. Each weak learner's prediction is multiplied by a factor that represents its contribution to the ensemble. The weights are usually determined based on the performance or importance of each weak learner.\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively learn from the mistakes of the ensemble and make incremental improvements. By combining weak learners in a sequential manner and focusing on the residuals or gradients, the algorithm aims to create a strong learner that can accurately capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe06b09-7c67-442e-bebd-981b31ef5708",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's a step-by-step explanation of how the ensemble is constructed:\n",
    "\n",
    "Initialize the ensemble: The algorithm starts by initializing the ensemble with a simple model, typically with the average or a constant value of the target variable. This serves as the initial prediction for all the data points.\n",
    "\n",
    "Calculate the residuals: The algorithm calculates the residuals or errors by taking the difference between the actual target values and the current predictions of the ensemble. The residuals represent the remaining errors that need to be reduced.\n",
    "\n",
    "Train a weak learner: A weak learner, often a decision tree with limited depth or complexity, is trained to fit the residuals. The weak learner is trained to predict the residuals as accurately as possible, focusing on areas where the ensemble is making the most mistakes.\n",
    "\n",
    "Update the ensemble: The predictions of the weak learner are multiplied by a learning rate (also known as the step size) and added to the current predictions of the ensemble. This update step is performed to gradually improve the predictions by reducing the residuals.\n",
    "\n",
    "Repeat steps 2-4: Steps 2 to 4 are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, the algorithm calculates the residuals based on the current predictions, trains a new weak learner to fit the residuals, and updates the ensemble.\n",
    "\n",
    "Final ensemble prediction: The final prediction is obtained by summing the predictions of all the weak learners in the ensemble. Each weak learner's prediction is weighted by a factor that represents its contribution to the ensemble. The weights are typically determined based on the performance or importance of each weak learner.\n",
    "\n",
    "By sequentially adding weak learners and updating the ensemble based on the residuals, the Gradient Boosting algorithm iteratively improves the predictions and reduces the overall errors. The weak learners focus on learning the remaining patterns or mistakes made by the ensemble, enabling the model to capture complex relationships and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb46db-6096-49fc-b571-5312eb07b6a7",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "The mathematical intuition behind the Gradient Boosting algorithm involves several key steps. Here's a high-level overview of the steps involved:\n",
    "\n",
    "Define the Loss Function: The first step is to define a suitable loss function that measures the error between the model's predictions and the true target values. The choice of the loss function depends on the specific problem (e.g., regression, classification) and the desired properties of the model.\n",
    "\n",
    "Initialize the Ensemble: The ensemble is initialized with an initial prediction, which is often a simple value such as the average or a constant value of the target variable. This serves as the starting point for subsequent iterations.\n",
    "\n",
    "Calculate the Negative Gradient: The negative gradient of the loss function with respect to the current predictions is calculated. This gradient represents the direction of steepest descent, indicating how the predictions should be updated to minimize the loss function.\n",
    "\n",
    "Train a Weak Learner: A weak learner, typically a decision tree with limited depth, is trained to fit the negative gradient or the residuals. The weak learner's task is to learn the corrections needed to improve the predictions based on the negative gradient.\n",
    "\n",
    "Update the Ensemble: The predictions of the weak learner are multiplied by a learning rate (also known as the step size) and added to the current predictions of the ensemble. This update step gradually improves the predictions by reducing the errors.\n",
    "\n",
    "Repeat Steps 3-5: Steps 3 to 5 are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, the negative gradient is recalculated based on the current predictions, a new weak learner is trained to fit the negative gradient, and the ensemble is updated.\n",
    "\n",
    "Final Ensemble Prediction: The final prediction is obtained by summing the predictions of all the weak learners in the ensemble. Each weak learner's prediction is weighted by a factor that represents its contribution to the ensemble.\n",
    "\n",
    "The mathematical intuition of the Gradient Boosting algorithm lies in the iterative process of updating the predictions based on the negative gradient and training weak learners to learn the residual errors. By sequentially adding weak learners and adjusting the ensemble's predictions, the algorithm gradually reduces the overall loss and improves the accuracy of the model. The final ensemble prediction combines the knowledge and corrections learned by the weak learners, resulting in a strong model with high predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
